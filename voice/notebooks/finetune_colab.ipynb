{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediRep Voice AI - Fine-Tuning with Unsloth\n",
    "\n",
    "**Before running:**\n",
    "1. Upload `train.jsonl` and `val.jsonl` from `voice/data/final/`\n",
    "2. Set Runtime > Change runtime type > T4 GPU (free) or A100 (faster)\n",
    "\n",
    "**Training data:** 1872 conversations with tool-calling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install Unsloth\n# Install from PyPI (stable) - NOT from git (git main requires unreleased unsloth_zoo)\n!pip install unsloth -q\n!pip install \"unsloth_zoo==2026.1.4\" -q\n!pip install --no-deps trl peft accelerate bitsandbytes -q\n\n# Verify install\nimport unsloth\nprint(f\"Unsloth version: {unsloth.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Base Model\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Llama-3.2-3B - fits in T4 GPU (16GB), good quality\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # Reduces memory 4x\n",
    ")\n",
    "\n",
    "print(f\"Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Add LoRA Adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=32,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\n\nprint(\"LoRA adapters added (r=32)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Upload and Load Dataset\n",
    "from google.colab import files\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Upload train.jsonl and val.jsonl from voice/data/final/\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"validation\": \"val.jsonl\"\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Format Dataset - KEEP tool_result so model learns to read them\nimport json\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files={\n    \"train\": \"train.jsonl\",\n    \"validation\": \"val.jsonl\"\n})\n\n# Noisy fields to strip from tool results\nNOISE_FIELDS = [\n    \"id\", \"scheme_id\", \"created_at\", \"updated_at\",\n    \"procedure_name_normalized\", \"data_source\"\n]\n\ndef format_chat(example):\n    messages = example[\"conversations\"]\n\n    cleaned = []\n    for m in messages:\n        if m[\"role\"] == \"tool_result\":\n            try:\n                data = json.loads(m[\"content\"])\n                for key in NOISE_FIELDS:\n                    data.pop(key, None)\n                # Tool result as a user message so chat template accepts it\n                cleaned.append({\"role\": \"user\", \"content\": f\"[Tool Result]: {json.dumps(data)}\"})\n            except:\n                cleaned.append({\"role\": \"user\", \"content\": f\"[Tool Result]: {m['content'][:500]}\"})\n        else:\n            cleaned.append(m)\n\n    text = tokenizer.apply_chat_template(cleaned, tokenize=False, add_generation_prompt=False)\n    return {\"text\": text}\n\ndataset = dataset.map(format_chat)\nprint(\"Dataset formatted WITH tool results\")\nprint(f\"Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")\nprint(f\"Sample:\\n{dataset['train'][0]['text'][:800]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Training Configuration\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nimport transformers\ntransformers.integrations.integration_utils.is_wandb_available = lambda: False\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=20,\n        num_train_epochs=5,\n        learning_rate=1e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"steps\",\n        save_steps=100,\n    ),\n)\n\nprint(\"Trainer configured (5 epochs, lr=1e-4, r=32)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train!\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"Training complete! Loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test the Model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the PM-JAY rate for knee replacement?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Test response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export to GGUF (for local deployment)\n",
    "print(\"Exporting to GGUF format...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"medirep-voice\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"  # Good balance of size/quality\n",
    ")\n",
    "print(\"Exported to medirep-voice/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download the Model\n",
    "!zip -r medirep-voice.zip medirep-voice/\n",
    "files.download(\"medirep-voice.zip\")\n",
    "print(\"Download complete! Extract and use with llama.cpp or llama-cpp-python\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}